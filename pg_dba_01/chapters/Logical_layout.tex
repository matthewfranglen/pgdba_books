\chapter{The logical layout}
\label{cha:LOGICLAY}\index{Logical layout}
In this we'll take a look to the PostgreSQL logical layout.
We'll start with the connection process. Then we'll see the logical relations like tables, indices
and views. The chapter will end with the tablespaces and the MVCC.

\section{The connection}
When a client starts a connection to a running cluster, the process pass through few steps. \newline

The first connection's stage is the check using the host based authentication. The cluster scans the
pg\_hba.conf file searching a  match for the connection's parameters. Those are, for example, the
client's host, the user etc. The host file is usually saved inside the the data area alongside the
configuration file postgresql.conf. The pg\_hba.conf is read from the top to the bottom and the
first matching row for the client's parameters is used to determine the authentication method to
use. If PostgreSQL reaches the end of the file without match the connection is refused.\newline

The pg\_hba.conf structure is shown in \ref{tab:PGHBA}

\begin{table}[H]
    \begin{tabular}{ccccc}
        Type & Database & User & Address & Method \\
        \hline
        local & name & name & ipaddress/network mask & trust\\
        host & * & * & host name & reject\\
        hostssl & &  &  & md5\\
        hostnossl & &  &  & password \\
                  & & &  & gss \\
                  & & &  & sspi \\
                  & & &  & krb5 \\
                  & & &  & ident \\
                  & & &  & peer \\
                  & & &  & pam \\
                  & & &  & ldap \\
                  & & &  & radius \\
                  & & &  & cert \\
    \end{tabular}
    \caption{\label{tab:PGHBA}pg\_hba.conf}
\end{table}

The column type specifies if the connection is local or host. The former is when the connection is
made using a socket. The latter when the connection uses the network. It's also possible to
specify if the host connection should be secure or plain using hostssl and hostnossl.\newline

The Database and User columns are used to match specific databases and users.\newline

The column address have sense only if the connection is host, hostssl or hostnossl. The value can
be an ip address plus the network mask. Is also possible to specify the hostname. There is the
full support for ipv4 and ipv6.

The pg\_hba.conf's last column is the authentication method for the matched row. The action to
perform after the match is done. PostgreSQL supports many methods ranging from the plain password
challenge to kerberos.\newline

We'll now take a look to the built in methods.

\begin{itemize}
    \item \textbf{trust}: The connection is authorised without any further action. Is quite useful
        if the password is lost. Use it with caution.

    \item \textbf{peer}: The connection is authorised if the OS user matches the
        database user. It's useful for the local connections.

    \item \textbf{password}: The connection establishes if the connection's user and the password
        matches with the values stored in the pg\_shadow system table. This method sends the password in
        clear text. Should be used only on trusted networks.

    \item \textbf{md5}: This method is similar to password. It uses a better security encoding the
        passwords using the md5 algorithm. Because md5 is deterministic, there is pseudo random
        subroutine which prevents to have the same string sent over the network.

    \item \textbf{reject}: The connection is rejected. This method is very useful to keep the sessions
        out of the database. e.g. maintenance requiring single user mode.

\end{itemize}

When the connection establishes the postgres main process forks a new backend process attached to
the shared buffer. The fork process is expensive. This makes the connection a potential
bottleneck. Opening new connections can degrade the operating system performance and eventually
produce zombie processes. Keeping the connections constantly connected maybe is a reasonable fix.
Unfortunately this approach have a couple of unpleasant side effects.\newline

Changing any connection related parameter like the max\_connections, requires a cluster restart.
For this reason planning the resources is absolutely vital. For each connection present in
max\_connections the cluster allocates 400 bytes of shared memory. For each connection established
the cluster allocates a per user memory area wich size is determined by the parameter
work\_mem.\newline

For example let's consider a cluster with a shared\_buffer set to 512 MB and the work\_mem
set to 100MB. Setting the max\_connections to only 500 requires a potentially 49 GB of total memory
if all the connections are in use. Because the work\_mem can affects the performances, its
value should be determined carefully. Being a per user memory any change to work\_mem does not
require the cluster's start but a simple reload.\newline

In this kind of situations a connection pooler can be a good solutions. The sophisticated
\href{http://www.pgpool.net/}{pgpool}  or the
lightweight \href{http://pgfoundry.org/projects/pgbouncer/}{pgbouncer}  can help to boost the
connection's performance.\newline

By default a fresh data area initialisation listens only on the localhost. The GUC parameter
governing this aspect is listen\_addresses. In order to have the cluster accepting connections from
the rest of the network the values should change to the correct listening addresses specified
as values separated by commas. It's also possible to set it to * as wildcard.

Changing the parameters max\_connections and listen\_addresses require the cluster restart.



\section{Databases}
\label{sec:DATABASES}
Unlikely other DBMS, a PostgreSQL connection requires the database name in the connection string.
Sometimes this can be omitted in psql when this information is supplied in another way.\newline

When omitted psql checks if the environment variable \$PGDATABASE \index{\$PGDATABASE variable} is
set. If \$PGDATABASE is missing then psql defaults the database name to connection's username. This
leads to confusing error messages. For example, if we have a username named test but not a database
named test the connection will fail even with the correct credentials.

\begin{verbatim}
postgres@tardis:~$ psql -U test -h localhost
Password for user test:
psql: FATAL:  database "test" does not exist
\end{verbatim}

This error appears because the pg\_hba.conf allow the connection for any database. Even for a not
existing one. The connection is then terminated when the backend ask to connect to the database
named test which does not exists.\newline

This is very common for the new users. The solution is incredibly simple because in a PostgreSQL
cluster there are at least three databases. Passing the name template1 as last parameter will do
the trick.

\begin{verbatim}
postgres@tardis:~$ psql -U test -h localhost template1
Password for user test:
psql (9.3.4)
SSL connection (cipher: DHE-RSA-AES256-SHA, bits: 256)
Type "help" for help.
\end{verbatim}

When the connection is established we can query the system table pg\_database to get the
cluster's database list.

\begin{lstlisting}[style=pgsql]
template1=> SELECT datname FROM pg_database;
    datname
---------------
 template1
 template0
 postgres
(3 rows)

\end{lstlisting}

Database administrators coming from other DBMS can be confused by the postgres database.
This database have nothing special. Its creation was added since the version 8.4 because it was
useful to have it. You can just ignore it or use it for testing purposes. Dropping the postgres
database does not corrupts the cluster. Because this database is often used by third party
tools before dropping it check if is in use in any way.\newline

The databases template0 and template1 \index{template1 database} \index{template0
    database} like the name suggests are the template databases. A template database \index{template
database} is used to build new database copies via the physical file copy.

When initdb initialises the data area the database template1 is populated with the correct
references to the WAL records, the system views and the procedural language PL/PgSQL. When
this is done the database template0 and the postgres databases are then created using the template1
database.

The database template0 doesn't allow the connections. It's main usage is to rebuild the
database template1 if it gets corrupted or for creating databases with a character encoding/ctype,
different from the cluster wide settings.
\index{CREATE DATABASE}

\begin{lstlisting}[style=pgsql]
postgres=# CREATE DATABASE db_test WITH ENCODING 'UTF8' LC_CTYPE 'en_US.UTF-8';
ERROR:  new LC_CTYPE (en_US.UTF-8) is incompatible with the LC_CTYPE of the
template database (en_GB.UTF-8)
HINT:  Use the same LC_CTYPE as in the template database, or use template0 as
template.

postgres=# CREATE DATABASE db_test WITH ENCODING 'UTF8' LC_CTYPE 'en_US.UTF-8'
TEMPLATE template0;
CREATE DATABASE
postgres=#

\end{lstlisting}

If the template is omitted the CREATE DATABASE statement will use template1 by default.


A database can be renamed or dropped with ALTER DATABASE and DROP DATABASE \index{ALTER
DATABASE}\index{DROP DATABASE} statements. Those operations require the exclusive access to the
affected database. If there are connections established the drop or rename will fail.

\begin{lstlisting}[style=pgsql]
postgres=# ALTER DATABASE db_test RENAME TO db_to_drop;
ALTER DATABASE

postgres=# DROP DATABASE db_to_drop;
DROP DATABASE

\end{lstlisting}




\section{Tables}\index{Tables}
\label{sec:TABLES}
In our top down approach to the PostgreSQL's logical model, the next step is the relation.
In the PostgreSQL jargon a relation is an object which carries the data or the way to
retrieve the data. A relation can have a physical counterpart or be purely logical. We'll take a
look in particular to three of them starting with the tables.\newline

A table is the fundamental storage unit for the data. PostgreSQL implements many kind of tables
with different levels of durability. A table is created using the SQL command CREATE TABLE. The data
is stored into a table without any specific order. Because the MVCC implementation a row update can
change the row's physical position. For more informations look to \ref{sec:MVCC}. PostgreSQL
implements three kind of tables.

\subsection{Logged tables}\index{Logged tables}
By default CREATE TABLE creates a logged table. This kind of table implements the durability
logging any change to the write ahead log. The data pages are loaded in the shared buffer and any
change to them is logged first to the WAL. The consolidation to the the data file happens later.

\subsection{Unlogged tables}\index{Unlogged tables}
\label{sub:UNLOGGEDTABLES}
An unlogged table have the same structure like the logged table. The difference is such kind of
tables are not crash safe. The data is still consolidated to the data file but the pages modified
in memory do not write their changes to the WAL. The main advantage is the write operations which
are considerably faster at the cost of the data durability. The data stored into an ulogged table
should be considered partially volatile. The database will truncate those tables when the crash
recovery occurs. Because the unlogged table don't write to the WAL, those tables are not accessible
on a physical standby.

\subsection{Temporary tables}\index{Temporary tables}
A temporary table is a relation which lives into the backend's local memory. When the connection
ends the table is dropped. Those table can have the same name for all the sessions because
they are completely isolated. If the amount of data stored into the table is lesser than
the temp\_buffers value the table will fit in memory with great speed advantage. Otherwise the
database will create a temporary relation on disk. The parameter temp\_buffers can be altered for
the session but only before the first temporary table is created.


\subsection{Foreign tables}\index{Foreign tables}
The foreign tables were first introduced with PostgreSQL 9.1 as read only relations, improving
considerably the DBMS connectivity with other data sources. A foreign table works exactly like a
local table. A foreign data wrapper interacts with the foreign data source and handles the
data flow.\newline

There are many different foreign data wrappers available for very exotic data sources. From the
version 9.3 the postgres\_fdw becomes available and the the foreign tables are writable. The
implementation of the postgres\_fdw implementation is similar to old dblink module with a more
efficient performance management and the connection's caching.

\section{Table inheritance}\index{Table inheritance}
PostgreSQL is an Object Relational Database Management System rather a simple DBMS. Some of the
concepts present in the object oriented programming are implemented in the PostgreSQL logic. The
relations are also known as classes and the table's columns as attributes. \newline

The table inheritance is a logical relationship between a parent table and one or more child
tables. The child table inherits the parent's attribute structure but not the physical
storage.\newline


Creating a parent/child structure is straightforward.

\begin{lstlisting}[style=pgsql]
db_test=#CREATE TABLE t_parent
                      (
                          i_id_data     integer,
                          v_data        character varying(300)
                      );

CREATE TABLE

db_test=#CREATE TABLE t_child_01

                      ()
             INHERITS (t_parent)
                      ;
db_test=# \d t_parent
            Table "public.t_parent"
  Column   |          Type          | Modifiers
-----------+------------------------+-----------
 i_id_data | integer                |
 v_data    | character varying(300) |
Number of child tables: 1 (Use \d+ to list them.)

db_test=# \d t_child_01
           Table "public.t_child_01"
  Column   |          Type          | Modifiers
-----------+------------------------+-----------
 i_id_data | integer                |
 v_data    | character varying(300) |
Inherits: t_parent

\end{lstlisting}

The inheritance is usually defined at creation time. It's possible to enforce the inheritance
between two existing tables with the ALTER TABLE ... INHERIT command. The two table's structure
must be identical.

\begin{lstlisting}[style=pgsql]

db_test=# ALTER TABLE t_child_01 NO INHERIT t_parent;
ALTER TABLE
db_test=# ALTER TABLE t_child_01 INHERIT t_parent;
ALTER TABLE

\end{lstlisting}

Because the physical storage is not shared then the unique constraints aren't globally enforced on
the inheritance tree. This prevents the creation of any global foreign key. Using the table
inheritance, combined with the constraint exclusion and the triggers/rules, is a partial workaround
for the table partitioning.

\section{Indices}
An index is a structured relation. The indexed entries are used to access the tuples stored in the
tables. The index entries are the actual data with a pointer to the corresponding table's pages.\newline

It's important to bear in mind that the indices add overhead to the write operations. Creating an index
does not guarantee its usage.The cost based optimiser, for example, can simply consider the index access
more expensive than a sequential access. The stats system views, like the pg\_stat\_all\_indexes,
store the usage counters for the indices.\newline

For example this simple query finds all the indices in the public schema with index scan counter zero.

\begin{lstlisting}[style=pgsql]
SELECT
        schemaname,
        relname,
        indexrelname,
        idx_scan
FROM
         pg_stat_all_indexes
WHERE
                schemaname='public'
        AND     idx_scan=0
;

\end{lstlisting}

Having an efficient maintenance plan can improve sensibly the database performance. Take a look to
\ref{cha:MAINTENANCE} for more information.

PostgreSQL implements many kind of indices. The keyword USING specifies the index type at create time.

\begin{lstlisting}[style=pgsql]
 CREATE INDEX idx_test ON t_test USING hash (t_contents);
\end{lstlisting}

If the clause USING is omitted the index defaults to the B-tree.

\subsection{b-tree}
The general purpose B-tree\index{index,b-tree} index implements the Lehman and Yao's
high-concurrency B-tree management algorithm. The B-tree can handle equality and range queries
returning ordered data. The indexed values are stored into the index pages with the pointers to the
table's pages. Because the index is a relation not TOASTable the max length for an indexed key is 1/3
of the page size. More informations about TOAST are in \ref{sec:TOAST} \newline

\subsection{hash}
The hash indices\index{index,hash} can handle only equality and are not WAL logged. Their changes
are not replayed if the crash recovery occurs and do not propagate to the standby servers.\newline

\subsection{GiST}
The GiST indices\index{index,GiST} are the Generalised Search Tree. The GiST is a collection of
indexing strategies organised under a common infrastructure. They can implement arbitrary indexing
schemes like B-trees, R-trees  or other. The default installation comes with operator classes working on
two elements geometrical data and for the nearest-neighbour searches. The GiST indices do not perform an
exact match. The false positives are removed with second rematch on the table's data.\newline

\subsection{GIN}
The GIN indices \index{index,GIN} are the Generalised Inverted Indices. This kind of index
is optimised for indexing the composite data types, arrays and vectors like the full text search
elements. This is the only index supported by the range types. The GIN are exact indices, when scanned
the returned set doesn't require recheck.

\section{Views}
\index{views}
\label{sec:VIEWS}
A view is a relation composed by a name and a query definition. This permits a faster access to complex
SQL. When a view is created the query is validated and all the objects involved are translated to their
binary representation. All the wildcards are expanded to the corresponding field's list.\newline

A simple example will help us to understand better this important concept. Let's create a table
populated using the function generate\_series(). We'll then create a view with a simple SELECT * from the
original table.

\begin{lstlisting}[style=pgsql]


CREATE TABLE t_data
        (
                i_id            serial,
                t_content       text
        );

ALTER TABLE t_data
ADD CONSTRAINT pk_t_data PRIMARY KEY (i_id);


INSERT INTO t_data
        (
                t_content
        )
SELECT
        md5(i_counter::text)
FROM
        (
                SELECT
                        i_counter
                FROM
                        generate_series(1,200) as i_counter
        ) t_series;

CREATE OR REPLACE VIEW v_data
AS
  SELECT
          *
  FROM
        t_data;


\end{lstlisting}

We can select from the view and from the the table with a SELECT and get the same data. The view's
definition in pg\_views shows no wildcard though.


\begin{lstlisting}[style=pgsql]
db_test=# \x
db_test=# SELECT * FROM pg_views where viewname='v_data';
-[ RECORD 1 ]--------------------
schemaname | public
viewname   | v_data
viewowner  | postgres
definition |  SELECT t_data.i_id,
           |     t_data.t_content
           |    FROM t_data;


\end{lstlisting}

If we add a new field to the table t\_data this will not applies to the view.

\begin{lstlisting}[style=pgsql]
 ALTER TABLE t_data ADD COLUMN d_date date NOT NULL default now()::date;

 db_test=# SELECT * FROM t_data LIMIT 1;
 i_id |            t_content             |   d_date
------+----------------------------------+------------
    1 | c4ca4238a0b923820dcc509a6f75849b | 2014-05-21
(1 row)


db_test=# SELECT * FROM v_data LIMIT 1;
 i_id |            t_content
------+----------------------------------
    1 | c4ca4238a0b923820dcc509a6f75849b
(1 row)



\end{lstlisting}

Using the statement CREATE OR REPLACE VIEW we can put the view in sync with the table's structure.

\begin{lstlisting}[style=pgsql]
 CREATE OR REPLACE VIEW v_data
AS
  SELECT
        *
  FROM
        t_data;

db_test=# SELECT * FROM v_data LIMIT 1;
 i_id |            t_content             |   d_date
------+----------------------------------+------------
    1 | c4ca4238a0b923820dcc509a6f75849b | 2014-05-21
(1 row)

\end{lstlisting}

Using the wildcards in the queries is a bad practice for many reasons. The potential outdated
match between the physical and the logical relations is one of those.\newline

The way PostgreSQL implements the views guarantee they never invalidate when the referred objects are
renamed. \newline

If new attributes needs to be added to the view the CREATE OR REPLACE statement can be used but only if
the fields are appended. If a table is referred by a view the drop is not possible. It's still possible
to drop a table with all the associated views using the clause CASCADE. This is a dangerous
practice though. The dependencies can be very complicated and a not careful drop can result in a
regression. The best approach is to check for the dependencies using the table pg\_depend.\newline

Storing a complex SQL inside the database avoid the overhead caused by the round trip between the client
and the server. A view can be joined with other tables or views. This practice is generally bad because
the planner can be confused by mixing different queries and can generate not efficient execution
plans.\newline

A good system to spot a view when writing a query is to use a naming convention. For example adding a v\_
in the view names and the t\_ in the table names will help the database developer to avoid
mixing logical an physical objects when writing SQL. Look to \ref{cha:COUPLETHINGS} for more information.

\index{view, updatable}
PostgreSQL from the version 9.3 supports the updatable views. This feature is limited just to the
simple views. A view is defined simple when the following is true.

\begin{itemize}


    \item  Does have exactly one entry in its FROM list, which must be a table or another updatable view.

    \item Does not contain WITH, DISTINCT, GROUP BY, HAVING,LIMIT, or OFFSET clauses at the top level.

    \item  Does not contain set operations (UNION, INTERSECT or EXCEPT) at the top level

    \item   All columns in the view's select list must be simple references to columns of the
        underlying relation. They cannot be expressions, literals or functions. System columns cannot be
        referenced, either.

    \item   Columns of the underlying relation do not appear more than once in the view's select list.

    \item   Does not have the security\_barrier property.

\end{itemize}

A complex view can still become updatable using the triggers or the rules.\newline

\index{view, materialised}
Another feature introduced by the 9.3 is the materialised views. This acts like a physical snapshot of
the saved SQL. The view's data can be refreshed with the statement REFRESH MATERIALIZED VIEW.


\section{Tablespaces}\index{tablespaces,logical}
\label{sub:TBS-LOGICAL}
A tablespace\index{tablespace} is a logical name pointing to a physical location.
This feature was introduced with the release 8.0 and its implementation did not change too much since
then. From the version 9.2  a new function pg\_tablespace\_location(tablespace\_oid) offers the
dynamic resolution of the physical tablespace location,making the dba life easier.\newline

When a new physical relation is created without tablespace indication, the value defaults to the
parameter default\_tablespace. If this parameter is not set then the relation's tablespace is set to
the database's default tablespace.
Into a fresh initialised cluster there are two tablespaces initially. One is the pg\_default which
points to the path \$PGDATA/base. The second is pg\_global which is reserved for the cluster's shared
objects and its physical path is \$PGDATA/global.\newline

Creating a new tablespace is very simple. The physical location must be previously created and the os
user running the postgres process shall be able to write into it. Let's create, for example, a tablespace
pointing to the folder named /var/lib/postgresql/pg\_tbs/ts\_test. Our new tablespace will be named
ts\_test.

\begin{lstlisting}[style=pgsql]
CREATE TABLESPACE ts_test
OWNER postgres
LOCATION '/var/lib/postgresql/pg_tbs/ts_test' ;

\end{lstlisting}

Only superusers can create tablespaces. The clause OWNER is optional and if  is omitted the tablespace's
owner defaults to the user issuing the command. The tablespaces are cluster wide and are listed into the
pg\_tablespace system table.\newline

The clause TABLESPACE followed by the tablespace name will create the new relation into the specified
tablespace.

\begin{lstlisting}[style=pgsql]
CREATE TABLE t_ts_test
        (
                i_id serial,
                v_value text
        )
TABLESPACE ts_test ;

\end{lstlisting}

A relation can be moved from a tablespace to another using the ALTER command. The following command
moves the table t\_ts\_test from the tablespace ts\_test to pg\_default.

\begin{lstlisting}[style=pgsql]
ALTER TABLE t_ts_test SET TABLESPACE pg_default;
\end{lstlisting}

The move is transaction safe but requires an access exclusive lock on the affected relation. The lock
prevents accessing the relation's data for the time required by the move.If the relation have a
significant size this could result in a prolonged time where the table's data is not accessible.
The exclusive lock conflicts any running pg\_dump which prevents any tablespace change.\newline


A tablespace can be removed with DROP TABLESPACE command but must be empty before the drop. There's no
CASCADE clause for the DROP TABLESPACE command.

\begin{lstlisting}[style=pgsql]
postgres=# DROP TABLESPACE ts_test;
ERROR:  tablespace "ts_test" is not empty

postgres=# ALTER TABLE t_ts_test SET TABLESPACE pg_default;
ALTER TABLE
postgres=# DROP TABLESPACE ts_test;
DROP TABLESPACE

\end{lstlisting}


A careful design using the tablespaces, for example putting tables and indices on different
devices,can improve sensibly the cluster's performance.\newline

In \ref{sub:TBS-PHYSICAL} we'll take a look to the how PostgreSQL implements the tablespaces from the
physical point of view.


\section{Transactions}
\label{sec:TRANSACTION}
\index{transactions}
PostgreSQL implements the atomicity, the consistency and the isolation with the MVCC\index{MVCC}.
The Multi Version Concurrency Control\index{Multi Version Concurrency Control}offers high efficiency in
the concurrent user accesss.\newline

The MVCC logic is somewhat simple. When a transaction starts a write operation gets a transaction id,
called XID,\index{XID} a 32 bit quantity. The XID value is used to determine the transaction's
visibility, its relative position in an arbitrary timeline. All the transactions with XID smaller than
the current XID in committed status are considered in the past and then visible. All the transactions
with XID bigger than the current XID are in the future and therefore invisible.\newline

The check is made at tuple level using two system fields xmin and xmax. When a new tuple is created the
xmin is set with the transaction's xid. This field is also referred as the insert's transaction id. When
a tuple is deleted then the xmax value is updated to the delete's xid. The xmax field is also know as
the delete's  transaction id. The tuple is not physically removed in order to ensure the read consistency
for any transaction in the tuple's past. The tuples having the xmax not set are live tuples. Therefore
the tuples which xmax is set are dead tuples. In this model there is no field dedicated to the
update which is an insert insert combined with a delete. The update's transaction id is used either for
the new tuple's xmin and the old tuple's xmax.\newline

The dead tuples are removed by VACUUM when no longer required by existing transactions.
For the tuple detailed description check \ref{sec:TUPLES}.\newline

Alongside with xmin and xmax there are cmin and cmax which data type is the command id, CID. Those fields
store the internal transaction's commands in order to avoid the command to be executed on the same tuple
multiple times. One practical effect of those fiels is to solve the database's Halloween Problem
described there \href{http://en.wikipedia.org/wiki/Halloween_Problem}{
http://en.wikipedia.org/wiki/Halloween\_Problem}.\newline

The SQL standard defines four level of the transaction's isolation. Each level allows or deny the
following transaction's anomalies.
\index{transactions, isolation levels}

\begin{itemize}
    \item \textbf{dirty read}\index{dirty read}, when a transaction can access the data written by a
        concurrent not committed transaction.

    \item \textbf{non repeatable read}\index{non repeatable read}, when a transaction repeats a previous read
        and finds the data changed by another transaction which has committed since the initial read.

    \item \textbf{phantom read}\index{phantom read}, when a transaction executes a previous query and
        finds a different set of rows with the same search condition because the results was changed by another
        committed transaction

\end{itemize}

The table \ref{tab:TRNISOLATION} shows the transaction's isolation levels and which anomalies are
possible or not within. PostgreSQL supports the minimum isolation level to read committed. Setting the
isolation level to read uncommited does not cause an error. However, the system adjusts silently the
level to read committed.

\begin{table}[H]
    \begin{tabular}{cccc}
        Isolation Level & Dirty Read    &    Nonrepeatable Read   &   Phantom
        Read\\
        \hline
        Read uncommitted  &  Possible    &    Possible     &   Possible\\
        Read committed    &  Not possible &  Possible     &   Possible\\
        Repeatable read   &  Not possible  & Not possible  &  Possible\\
        Serializable      &  Not possible  & Not possible   & Not possible\\
    \end{tabular}
    \caption{\label{tab:TRNISOLATION}SQL Transaction isolation levels}
\end{table}

The isolation level can be set per session with the command SET TRANSACTION ISOLATION LEVEL.
\begin{lstlisting}[style=pgsql]
SET TRANSACTION ISOLATION LEVEL { SERIALIZABLE | REPEATABLE READ | READ
COMMITTED | READ UNCOMMITTED };
\end{lstlisting}

It's also possible to change the isolation level cluster wide changing the GUC
parameter transaction\_isolation.

\subsection{Snapshot exports}
\label{sub:SNAPEXPORT}\index{transactions, snapshot export}
PostgreSQL 9.2 introduced the transaction's snapshot exports. A session with an open transaction, can
export its snapshot to other sessions. The snapshot can be imported as long as the exporting transaction
is in progress. This feature opens some interesting scenarios where multiple backends can import a
consistent snapshot and run, for example, read queries in parallel. One brilliant snapshot export's
implementation is the parallel export available with the 9.3's pg\_dump. Check \ref{sec:PGDUMPINT} for
more information.\newline

An example will help us to explain better the concept. We'll use the table created in \ref{sec:VIEWS}.
The first thing to do is connecting to the cluster and start a new transaction with at least the
REPEATABLE READ isolation level. Then the function pg\_export\_snapshot() is used to get the
snapshot's identifier.

\begin{lstlisting}[style=pgsql]
postgres=# BEGIN TRANSACTION ISOLATION LEVEL REPEATABLE READ;
BEGIN
postgres=# SELECT pg_export_snapshot();
 pg_export_snapshot
--------------------
 00001369-1
(1 row)

postgres=# SELECT count(*) FROM t_data;
 count
-------
   200
(1 row)

\end{lstlisting}

Connectin with another backend let's remove all the rows from the table t\_data table.

\begin{lstlisting}[style=pgsql]
postgres=# DELETE FROM t_data;
DELETE 200
postgres=# SELECT count(*) FROM t_data;
 count
-------
     0
(1 row)

\end{lstlisting}

After importing the snapshot 00001369-1 the rows are back in place.

\begin{lstlisting}[style=pgsql]
postgres=# BEGIN TRANSACTION ISOLATION LEVEL REPEATABLE READ;
BEGIN
postgres=# SET TRANSACTION SNAPSHOT '00001369-1';
SET
postgres=# SELECT count(*) FROM t_data;
 count
-------
   200
(1 row)

\end{lstlisting}


It's important to use at least the REPEATABLE READ as isolation level. Using the READ COMMITTED for the
export does not generates an error. However the snapshot is discarded immediately because the READ
COMMITTED takes a new snapshot for each command.
